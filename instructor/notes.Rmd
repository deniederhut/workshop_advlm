---
title: "Advanced Linear Modeling in R"
author: "Dillon Niederhut"
date: "November 11, 2015"
output: pdf_document
---

```{r, echo=FALSE}
# options(warn=-1)
knitr::opts_knit$set(root.dir = '../')
```

# Introduction

## Motivation

Most of what you'll want to do in inferential statistics is either a linear model, or based on a linear model. At one end of the spectrum, t-tests, ANCOVA, and linear constrasts are highly restricted versions of lienar models that allow easy calculation. At the other end of the spectrum, machine learning algorithms like logistic classifiers, ridge regression, and lasso regression, are all based on the repeated use of calculating full linear models. When used correctly, the linear model is one of the best ways to perform inferential statistics. When used incorrectly, the linear model is highly susceptible to spurious results (that lead to retracted papers!).

## Orientation

The linear model is one which assumes a deterministic world, where present observations are linear combinations of past observations.

# Contrasts

## Contrasts are useful for testing specific hypotheses

With an F-test, you can learn that a variable is causing a difference, but not what that difference is. You can think about contrasts as a way to look at the levels of a variable and ask which level, or groups of levels, are causing the difference. Maybe all of the levels are involved, but each one only a little bit. Contrasts allow you to test this possibility too. 

## The default contrast set for a factor is `contr.treatment`

This is the same thing as 'one-hot encoding'. R treats the first level of the variable as the basis for comparison, and then every subsequent level is represented as a binary 'dummy' variable.

```{r}
contr.treatment(4)
```

## If your data don't follow a treatment logic, you can try the sum or Helmert contrasts

```{r}
contr.helmert(4)
contr.sum(4)
```

## If your data have inherent order, try the polynomial contrast set

```{r}
contr.poly(4)
```

This produces linear and quadratic contrasts. It's a bit hard to see that though, right? This is because R is making all of the contrasts orthogonal for you. Non-orthogonal contrasts risk explaining the same bit of variance twice, and make you think that you are predicting more than you actually are.

## Assigning contrasts to a factor is easy

The syntax looks like this:

```{r}
my.factor = factor(c('Dillon', 'Andrew', 'Shinhye', 'Patty'))
contrasts(my.factor) <- contr.helmert
contrasts(my.factor)
```

## But you don't have to let R do it for you

The benefit to using R's built-ins is that they are guaranteed to be correct. But, you are all freethinking rebels (you are at Berkeley after all), so let's look at how to do this by hand.

```{r}
my.contrast <- matrix(c(
  -2, -1, 1, 2,
  0, 0, 1, 1,
  0, 1, 0, 1
  ), nrow=4, ncol=3)
contrasts(my.factor) <- my.contrast
contrasts(my.factor)
```

## Your turn!

Let's load in some real data from the data folder. This is a dataset containing information about different primates -  how large they are, how large their brains are, and how much energy they use in a day.

```{r}
dat <- read.csv('data/primate_energetics.csv')
```

Look at the variable `clade`. This variable contains five levels:

--------------|------------------
Strepsirrhini | lemurs and lorises
Platyrrhini | New World monkeys like capuchins and howlers
Cercopithecoidea | Old World monkeys like macaques and babboons
Hominoidea | apes like gibbons and gorillas
Homo | people like you

How would you construct a contrast for this variable?

# Robusticity

## Some assumptions of the linear model

1. Errors are normally distributed
2. Errors are independent
3. Variation is homoscedastic
4. X is measured without error

## Checking normality of errors

Suppose we were interested in predicting body size from brain size. We could construct a model like this:

```{r}
model.1 <- lm(W ~ BrainW, data=dat)
```

And then plot the residuals from the model against a normal distribution

```{r}
qqnorm(model.1$residuals)
qqline(model.1$residuals)
```

## Checking independence of errors

This one is pretty easy - you can simply regress your predictors on your residuals (or plot them)

```{r}
summary(lm(model.1$resid ~ BrainW, data=dat[!(is.na(dat$BrainW)), ]))
```

```{r}
library(ggplot2)
dat.1 <- dat[!(is.na(dat$BrainW)), ]
dat.1$resid <- model.1$residuals
ggplot(data=dat.1, aes(x=BrainW, y=resid)) + geom_jitter() + stat_smooth(method='lm')
```

## Checking homoscedasticity of errors

You can eyeball this by plotting your residuals against your predictor, and looking at rolling estimate of variance

```{r}
ggplot(data=dat.1, aes(x=BrainW, y=resid)) + geom_jitter() + stat_quantile()
```

A more formal test, called the Breusch-Pagan test, is found in the lmtest package. Essentially, it is just squaring the residuals and regressing your predictors on it.

```{r}
library(lmtest)
bptest(model.1)
```

## Checking for error in X

There isn't really a way to establish statistically that your predictors have been measured without error. The good news is that linear models are fairly robust to violations of this assumption already. The better news is that if you suspect your predictors have very large margins of error, you can use major axis regressions.

```{r}
library(lmodel2)
model.2 <- lmodel2(W ~ BrainW, data=dat, nperm=10)
model.2
```

## Linear models are very susceptible to outliers

This isn't really an assumption that we are violating so much as a consequence of parameterizing with means (highly susceptible to outliers) and variance (squared outliers). You can test for outliers with various measures of leverage, including:

1. DFBetas
2. DFFit
3. Cook's Distance

These can be calculated with:

```{r}
influence.measures(model.1)
```

If you find your model contains high leverage cases, you can either pluck them out by hand (preferably by selecting against something like Cook's Distace), or try running a regression build to be robust to outliers.

```{r}
library(robust)
model.3 <- lmRob(W ~ BrainW, data=dat)
summary(model.3)
```

Exluding the outliers in this case has brought the coefficient of our predictor close to zero. Can you guess why? If you aren't sure, plot it.

## Your turn!

So far we've been predicting body weight from brain weight. Try predicting body weight from brain weight and clade. Your base model should look like this:

```
  <- lm(W ~ BrainW + Clade, data=dat)
```

Rerun your regression diagnostics. What do you find?

# Stepwise Models and Uncertainty Reduction

## Fundamental constraints in prediction

We'll start with the bad news. Any kind of predicting method is constrained in how many independent variables you can include in the model (this is why machine learning algorithms always start with a dimensionality reduction method like PCA).

Let's do a little test to show how much of a problem this is. Run the code below a couple of times. How often do you get significant results? Keep in mind that these are randomly generated data, so we already know that there *shouldn't be* any significant predictors.

```{r}
dat.toy <- data.frame(value=runif(4), var1=runif(4), var2=runif(4))
summary(lm(value ~ var1 + var2, data=dat.toy))
```

R won't let us run `lm` with four variables, but if it did, we would be able to 100% accurately predict four random number from four sets of other random numbers. Overfitting, combined with p hacking and the file drawer problem, are responsible for the giant world of false findings and irreproducable science that you keep hearing about. Generally speaking, it is bad science to throw predictors haphazardly into a model and see what comes out; however, there are times when this approach is either required or useful in its own right.

## Weeding out predictors the smart way

It may happen to you in your life that you have no theory, hypothesis, or intuitions about how your predictors could be related to your outcome variable. In this case, you can weed out predictors using what is called a stepwise regression.

```{r}
library(MASS)
dat.omit <- na.omit(dat)
model.base <- lm(W ~ ., data=dat.omit)
model.4 <- stepAIC(model.base)
summary(model.4)
```

Doing it this way isn't particularly helpful - genera tend to be around the same body size, so really we are just recapitulating taxonomies here.

## Specifying forward stepwise models

Part of the problem is that backwards models tend to keep predictors that aren't particularly useful. The other part of the problem is that we threw every variable we had into the model.

```{r}
model.base <- lm(W ~ 1, data=dat.omit)
model.5 <- stepAIC(model.base, scope=list(upper = ~ W + BrainW + RMR + TEE), direction='forward')
summary(model.5)
```

## Comparing homogeneous models

Stepwise regression is evalued by maximizing the likelihood that a model will reduce your uncertainty about the dependent variable. This can't be assessed in an exact sense, but models that differ only the inclusion of a parameter can be compared my relative likelihood. This can be calculated with:

```{r}
p <- exp((model.5$anova$AIC[3] - model.5$anova$AIC[2]) / 2)
p
```

This is telling us that the simpler model is `r p` times as likely to minimize information loss as the more complex model. Or, to put it another way, the probability that the complex model is better is `r 1-p`.

# Comparing Heterogeneous Models

##

# Generalized Models

## 

